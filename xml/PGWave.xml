<?xml version="1.0" encoding="UTF-8"?>
<pipeline
   xmlns="http://glast-ground.slac.stanford.edu/pipeline"
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://glast-ground.slac.stanford.edu/pipeline https://glast-ground.slac.stanford.edu/Pipeline-II/schemas/2.2/pipeline.xsd">
<task name="PGWave" version="10.6" type="Data">
  <variables>
    <!-- Default values that can be overridden at the command line. -->

    <!-- variables for running at S3DF with containers -->
    <var name="preamble"></var> <!-- pre-amble for job executables -->
    <var name="container_image">/sdf/group/fermi/sw/containers/fermi-rhel6.sif</var>
    <var name="container_volumes">--bind /sdf:/sdf,/sdf/group/fermi/sw/package:/afs/slac.stanford.edu/package</var>
    <var name="container_wrap">singularity exec ${container_volumes} ${container_image}</var>
    <var name="extra"></var>  <!--extra batch options-->
    <var name="jobsite">S3DFDATA</var>
    <var name="GPL_XROOTD_DIR">/sdf/data/fermi/a/applications/xrootd/dist/v3.1.1/i386_rhel60/bin</var>
    <var name="SCRIPT_DIR">/sdf/data/fermi/a/ground/ASP/prod/ASP_at_S3DF/pipeline_scripts</var>

    <!-- original pipeline variables, modified for S3DF -->
    <var name="BINDIR">bin/redhat6-x86_64-64bit-gcc44-Optimized</var>
    <var name="datacatalog_imp">datacatalog</var>
    <var name="outputFolder">/ASP/Results/pgwave</var>
    <var name="xrootd_folder">/ASP/pgwave</var>
    <var name="GALPROP_MODEL">/sdf/data/fermi/a/ground/GLAST_EXT/diffuseModels/v5r0/gll_iem_v06.fits</var>
    <var name="logRoot">/sdf/group/fermi/ground/PipelineOutput/ASP/log_files</var>
  </variables>
  <prerequisites>
    <prerequisite name="folder" type="string"/>
    <prerequisite name="OUTPUTDIR" type="string"/>
    <prerequisite name="frequency" type="string"/>
    <prerequisite name="interval" type="integer"/>
    <prerequisite name="TSTART" type="integer"/>
    <prerequisite name="TSTOP" type="integer"/>
    <prerequisite name="ASP_PATH" type="string"/>
    <prerequisite name="CATDIR" type="string"/>
    <prerequisite name="ASP_PGWAVEROOT" type="string"/>
  </prerequisites>

  <process name="catalogQuery" site="${jobsite}">
    <job executable="bash ${SCRIPT_DIR}/datacat_query.sh" batchOptions="${extra}" />
  </process>

  <process name="getPgwInputData" autoRetryMaxAttempts="2" site="${jobsite}">
    <job executable="${preamble} ${container_wrap} ${ASP_PGWAVEROOT}/${BINDIR}/getPgwInputData.sh"
         batchOptions="${extra}"/>
    <depends>
      <after process="catalogQuery" status="SUCCESS"/>
    </depends>
  </process>

  <process name="runpgw" autoRetryMaxAttempts="2" site="${jobsite}">
    <job executable="${preamble} ${container_wrap} ${ASP_PGWAVEROOT}/${BINDIR}/runpgw.sh"
         batchOptions="${extra}"/>
    <depends>
      <after process="getPgwInputData" status="SUCCESS"/>
    </depends>
  </process>

  <process name="refinePositions" autoRetryMaxAttempts="2" site="${jobsite}">
    <job executable="${preamble} ${container_wrap} ${ASP_PGWAVEROOT}/${BINDIR}/refinePositions.sh"
         batchOptions="${extra}"/>
    <depends>
      <after process="runpgw" status="SUCCESS"/>
    </depends>
  </process>

  <process name="runsrcid" autoRetryMaxAttempts="2" site="${jobsite}">
    <job executable="${preamble} ${container_wrap} ${ASP_PGWAVEROOT}/${BINDIR}/runsrcid.sh"
         batchOptions="${extra}"/>
    <depends>
      <after process="refinePositions" status="SUCCESS"/>
    </depends>
  </process>

  <process name="createArchive" autoRetryMaxAttempts="1" site="${jobsite}">
    <job executable="${preamble} ${container_wrap} ${ASP_PGWAVEROOT}/${BINDIR}/asp_pgwave_createTarBall.sh"
         batchOptions="${extra}"/>
    <depends>
      <after process="runsrcid" status="SUCCESS"/>
    </depends>
  </process>

  <process name="registerData" site="${jobsite}">
    <script><![CDATA[
    print "using datacatalog implementation: ", datacatalog_imp
    datacat = eval(datacatalog_imp)
    streamId = pipeline.getStream()
    print "registering with stream ID = ", streamId
    #
    #  Register the archive file
    #
    createArchive = pipeline.getProcessInstance("createArchive")
    dataType = "PGW_ARCHIVE"

    filePath = createArchive.getVariable("tarball_name")

    attributes = (('nDatasetId=%s:nMetStart=%s:nMetStop=%s:' +
                   'sFrequency=%s:nInterval=%i') %
                  (streamId, TSTART, TSTOP, frequency, interval))
    print attributes

    outfile = filePath.split('/')[-1]
    logicalPath = '%s/%s:%s' % (outputFolder, dataType, outfile)
    print logicalPath

    if filePath.find('root') == 0:
        filePath += "@SLAC_XROOT"
    print filePath

    datacatalog.registerDataset(dataType, logicalPath, filePath,
                                attributes)
    #
    # Register the ASP data viewer files (currently still stored on nfs)
    #
    def filename(ext, suffix):
        fn = 'Filtered_evt' + ext
        tokens = fn.split('.')
        return tokens[0] + '_' + suffix + '.' + tokens[1]
    suffix = OUTPUTDIR.split('/')[-1]
    exts = ('.fits', '_map.fits', '_map.list', '_map.reg',
            '_map_pgw_out.fits', '_map_ait.gif', '_map_ait.png')
    files = [filename(ext, suffix) for ext in exts]
    dataTypes = ('EVENTS', 'SKYMAP', 'PGWAVESOURCELIST', 'DS9REGIONS',
                 'PGWAVESUMMARY', 'SKYMAP', 'SKYMAP')
    for outfile, dataType in zip(files, dataTypes):
        logicalPath = ('%s/%s/%s:%s'
                       % (outputFolder, frequency, dataType, outfile))
        print logicalPath
        filePath = '%s/%s' % (OUTPUTDIR, outfile)
        print filePath
        attributes = ('nDatasetId=%s:nMetStart=%i:nMetStop=%i'
                      % (streamId, TSTART, TSTOP))
        print attributes
        datacat.registerDataset(dataType, logicalPath,
                                filePath, attributes)
    # FT2 is a special case
    outfile = 'FT2_%s.fits' % suffix
    logicalPath = ('%s/%s/%s:%s'
                   % (outputFolder, frequency, 'FT2', outfile))
    print logicalPath
    filePath = '%s/%s' % (OUTPUTDIR, outfile)
    print filePath
    attributes = ('nDatasetId=%s:nMetStart=%i:nMetStop=%i'
                  % (streamId, TSTART, TSTOP))
    print attributes
    datacat.registerDataset('FT2', logicalPath, filePath, attributes)
    #
    if frequency in ('six_hours', 'daily', 'weekly'):
        pipeline.createSubstream("drpMonitoring", 0,
                                 "pgwave_streamId=%i" % streamId)
    ]]></script>
    <depends>
      <after process="createArchive" status="SUCCESS"/>
    </depends>
    <createsSubtasks>
      <subtask>drpMonitoring</subtask>
    </createsSubtasks>
  </process>

  <process name="clean_up_files" autoRetryMaxAttempts="1" site="${jobsite}">
    <job executable="${preamble} ${container_wrap} ${ASP_PGWAVEROOT}/${BINDIR}/asp_pgwave_clean_up_files.sh"
         batchOptions="${extra}"/>
    <depends>
      <after process="registerData" status="SUCCESS"/>
    </depends>
  </process>

  <task name="drpMonitoring" version="1.0" type="Data">
    <prerequisites>
      <prerequisite name="pgwave_streamId" type="integer"/>
    </prerequisites>
    <process name="launchDrpMonitoring" autoRetryMaxAttempts="1" site="${jobsite}">
      <job executable="bash ${SCRIPT_DIR}/launch_drp_monitoring.sh"
           batchOptions="${extra}"/>
    </process>
  </task><!--drpMonitoring-->

</task><!--PGWave-->
</pipeline>
